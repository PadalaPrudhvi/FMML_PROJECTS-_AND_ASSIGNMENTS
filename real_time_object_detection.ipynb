{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PadalaPrudhvi/FMML_PROJECTS-_AND_ASSIGNMENTS/blob/main/real_time_object_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a278abb",
      "metadata": {
        "papermill": {
          "duration": 0.015665,
          "end_time": "2023-07-16T01:54:11.284559",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.268894",
          "status": "completed"
        },
        "tags": [],
        "id": "9a278abb"
      },
      "source": [
        "# Real-Time Object Detection\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e90ee81",
      "metadata": {
        "papermill": {
          "duration": 0.014442,
          "end_time": "2023-07-16T01:54:11.314200",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.299758",
          "status": "completed"
        },
        "tags": [],
        "id": "2e90ee81"
      },
      "source": [
        "![Real-Time Object Detection](attachment:2d3614e8-8a30-4d11-a0f8-dc6fd9859d3a.png)\n",
        "##### Figure 1: An Illustration of Real-Time Object Detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a59d185",
      "metadata": {
        "papermill": {
          "duration": 0.014431,
          "end_time": "2023-07-16T01:54:11.343338",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.328907",
          "status": "completed"
        },
        "tags": [],
        "id": "5a59d185"
      },
      "source": [
        "Real-Time Object Detection (Boesch,2023) is the procedure of detecting and localizing objects in a video stream or a sequence of images in real time or nearly real time. Simply it identifies and classifies objects within a video or image frames when they are being captured or displayed.\n",
        "Object Detection is a crucial task for Autonomous Driving, Robots, Healthcare, Video Surveillance and for many areas.\n",
        "\n",
        "Mainly Computer Vision and Deep Learning approches are using for past years by researchers for real-time object detection. This is a challenging task because to get the timely accurate results they should focus on both speed and accuracy in a balanced way also about the available computational resources.\n",
        "\n",
        "The main requirement is to have quality image data to train the traditional vision models or the deep learning models in the desired area for object detection. Here I'm focusing on a specific area that real-time object detection is applying to find out what are the recent past two years advancements regarding real-time object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c67dd5f",
      "metadata": {
        "papermill": {
          "duration": 0.0148,
          "end_time": "2023-07-16T01:54:11.372802",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.358002",
          "status": "completed"
        },
        "tags": [],
        "id": "9c67dd5f"
      },
      "source": [
        "# Introduction\n",
        "Coral reefs also known as rainforests of the sea. They are immensely important to the world because of the numerous benifits they provide. Coral reefs are home to many different kinds of plants and animals. They provide food and livelihoods for people, protect coastlines from storms, attract tourists, and help regulate the climate. They are also a potential source of new medicines. It's important to take care of coral reefs to preserve their benefits for us and for the health of the oceans.\n",
        "\n",
        "When it comes to coral reefs, the Great Barrier Reef (GBR) is the World's Largest coral reef system. It is made up of small corals that form large structures. But the reef is under threat due to factors like climate change, pollution, and the Crown-of-Thorns Starfish. However, the GBR faces significant threats, and one of the main factors contributing to coral loss is the Crown-of-Thorns Starfish (COTS). The Crown-of-Thorns Starfish is a species of starfish that feeds on corals and has been a part of coral reef ecosystems for thousands of years. These starfish are about the size of a hat and are covered in poisonous spines. Under normal circumstances, the reef was able to recover from the natural life cycle of COTS outbreaks, as affected coral had time to regrow. However, in recent times, the GBR's resilience has been changed due to a combination of stress factors like climate change, pollution, and ocean acidification.\n",
        "\n",
        "When the population of COTS explodes, thousands of starfish can appear on individual reefs, consuming a significant portion of the coral. This feeding madness leaves behind a reef scarred with white patches, and it takes several years for the reef to recover. With the added stress factors affecting the health of the GBR, the natural recovery process is becoming more challenging.\n",
        "\n",
        "That's why australian agencies are trying to track and control COTS populations to ecologically sustainable levels."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f97daad",
      "metadata": {
        "papermill": {
          "duration": 0.014366,
          "end_time": "2023-07-16T01:54:11.402156",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.387790",
          "status": "completed"
        },
        "tags": [],
        "id": "2f97daad"
      },
      "source": [
        "![Crown-of-Thorns Starfish (COTS)](https://user-images.githubusercontent.com/51321172/183879687-e5201d27-5b78-4b88-8e2c-887a51110f2f.jpg)\n",
        "##### Figure 2: Crown-of-Thorns Starfish (COTS)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4821f2bd",
      "metadata": {
        "papermill": {
          "duration": 0.014436,
          "end_time": "2023-07-16T01:54:11.432052",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.417616",
          "status": "completed"
        },
        "tags": [],
        "id": "4821f2bd"
      },
      "source": [
        "# Related Research work\n",
        "One of the popular methods to coral cover and COTS surveillance is the Manta Tow method. A snorkel-diver is towed behind a boat to visually assess the underwater habitat. The boat stops at regular intervals to record observations on a datasheet. This method helps identify COTS outbreaks, monitor existing outbreaks, and informs decisions on deploying control teams. However as the first paper (Liu,2021) said there are few limitations to this approach like operational scalability, data resolution, reliability and traceability. Therefore the researchers involved have used underwater imaging devices (GoPro Hero9 cameras) to collect data from reefs with COTS Control teams in a COTS outbreak area at GBR. They have appreciated the Google/Kaggle team for supporting this work in various ways as well.\n",
        "\n",
        "They have worked with domain experts and marine scientists to validate that dataset annotations and shared the data with international community through a Kaggle competition to apply Machine Learning and AI technologies to analyze images for the  broadscale surveillance of underwater habitats.\n",
        "\n",
        "The object detection dataset contains only a single class, which is COTS (Crown-of-Thorns starfish). To find all COTS recall is valued. Therefore F2-score is used for the evaluation.\n",
        "\n",
        "As the second research paper (Heenaye-Mamode Khan,2023) I read, researchers have used three datasets available in Kaggle and their own COTS images captured in the lagoon around the island of Mauritius. Also they have used Data Augmentation techniques like rotation, flipping and distortion to increase and balance the dataset.\n",
        "\n",
        "They have conducted a dataset preprocessing, used You Only Look Once (YOLOv4) algorithm for Object Detection and classified images using another two pre-trained deep learning models VGG19 and MobileNetV2 also integrated an attention model to enahance the accuracy of COTS idetification.\n",
        "\n",
        "Working with underwater images is still a challenge in the Object Detection field. Therefore this demonstrates the use of deep learning techniques is the solution they have got some decent results.\n",
        "\n",
        "Also I found another most recent research paper (Lv,2023) about Vision Transformers beat YOLOs on real-time object detection. They are also deep learning models but originally proposed for natural language processing tasks. They works well in real-time object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2db4406",
      "metadata": {
        "papermill": {
          "duration": 0.014543,
          "end_time": "2023-07-16T01:54:11.461244",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.446701",
          "status": "completed"
        },
        "tags": [],
        "id": "c2db4406"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "\n",
        "I. Data Collection\n",
        "\n",
        "Here I'm using two different resources for my analysis about real-time object detection advancements.\n",
        "\n",
        "    - kaggle_writeups CSV file from 2023-kaggle report dataset.\n",
        "    - Three research papers from Google search results.\n",
        "    \n",
        "I have read research papers and have summed up it in the Related Research work section.\n",
        "\n",
        "You can find my thought process and the code in Appendix A that I used to find best 5 writeups to showcase the past two years advancements for the real-time object detection.\n",
        "\n",
        "\n",
        "II. Analysis of the first five winning solution writeups from Kaggle teams for the TensorFlow - Help Protect the Great Barrier Reef competition\n",
        "\n",
        "This is the competition (Kaggle,n.d.) that mentioned in the first paper (Liu,2021) the Australia’s National Science Agency, CSIRO has teamed up with Google to innovate solutions using new AI/ML technologies to detect Crown-of-Thorns Starfish in GBR. You can get the full detailed idea of the sequences of underwater images dataset in the competition Data tab on Kaggle.\n",
        "\n",
        "There were 2025 Kaggle teams have participated for the competition and you can see the following table of best 5 writeups summaries. I congratulate the winners and thank at the same time because not only they have learnt they have shared these techniques with us. We can assume that with the winning solutions to a competition like this, they're very talented and most updated for doing object detection using image/video data with current technologies.\n",
        "\n",
        "Let's look at what the winning Kagglers have learnt!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "594dfd57",
      "metadata": {
        "papermill": {
          "duration": 0.01577,
          "end_time": "2023-07-16T01:54:11.491645",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.475875",
          "status": "completed"
        },
        "tags": [],
        "id": "594dfd57"
      },
      "source": [
        "                           Table 1: Writeups Summarizations.\n",
        "![Table1](attachment:fe3d6f8f-e9ba-4c28-bd2b-c55d6b1ede45.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe824d75",
      "metadata": {
        "papermill": {
          "duration": 0.014294,
          "end_time": "2023-07-16T01:54:11.520505",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.506211",
          "status": "completed"
        },
        "tags": [],
        "id": "fe824d75"
      },
      "source": [
        "There are lots of models and techniques have used by winning Kagglers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcbc7d6b",
      "metadata": {
        "papermill": {
          "duration": 0.014135,
          "end_time": "2023-07-16T01:54:11.549373",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.535238",
          "status": "completed"
        },
        "tags": [],
        "id": "fcbc7d6b"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "The Kaggle teams have used many latest techniques including using YOLOv5, EfficientDet like deep learning models, ensembling, data augmentations...etc. They have used them instead of the past computer vision practices for object detection like Haar Cascades, Histogram of Oriented Gradients, Edge Detection and Speeded-Up Robust Features. That proves in the Heenaye-Mamode Khan (2023) research paper too. In the writing time of this essay these deep learning techniques have more improved. That can be seen in this article (Boesch,2023). YOLOv7 is there too. Also in the Lv (2023) research paper with vision transformers as well. Therefore definitely usage of these deep learning advancements are better than previous approches.\n",
        "\n",
        "So this essay about real-time object detection in the category of Image and/or Video data summarizes the rapid advancements in AI from the past two years that Kaggle community have learnt from experimenting."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fda010ec",
      "metadata": {
        "papermill": {
          "duration": 0.01423,
          "end_time": "2023-07-16T01:54:11.578214",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.563984",
          "status": "completed"
        },
        "tags": [],
        "id": "fda010ec"
      },
      "source": [
        "# Future work\n",
        "\n",
        "With the learnt advancements, ML community can apply these knowledge in new domains of real world applications. This understanding will help to create many advanced models like Attention-based models not only for Object Detection but also in Image Segmentation, Image Classification like computer vision tasks.\n",
        "\n",
        "Did you detect that in Figure 1 there was a perfect detection of objects without showing possibilities? Our AI industry is not that much capable of that yet and growing.  \n",
        "\n",
        "I hope the learnt advancements stated in this essay will help in contributing to the growth and progress for the entire machine learning eco-system and shaping the future of Artificial Intelligence in real world applications.\n",
        "\n",
        "Thank you for reading!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ca211a",
      "metadata": {
        "papermill": {
          "duration": 0.01425,
          "end_time": "2023-07-16T01:54:11.607024",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.592774",
          "status": "completed"
        },
        "tags": [],
        "id": "97ca211a"
      },
      "source": [
        "# References\n",
        "\n",
        "bestfitting. (n.d.). 5th place solution, poisson blending, detection and tracking. Retrieved from https://www.kaggle.com/c/31703/discussion/308007\n",
        "\n",
        "Boesch, G. (2023). Object Detection in 2023: The Definitive Guide. Retrieved from https://viso.ai/deep-learning/object-detection/\n",
        "\n",
        "Chenglu. (n.d.). 2nd Solution - YOLOv5. Retrieved from https://www.kaggle.com/c/31703/discussion/307760\n",
        "\n",
        "Ha, Q. (n.d.). Trust CV -- 1st Place Solution. Retrieved from https://www.kaggle.com/c/31703/discussion/307878\n",
        "\n",
        "Heenaye-Mamode Khan, M., Makoonlall, A., Nazurally, N., & Mungloo-Dilmohamud, Z. (2023, April 5). Identification of Crown of Thorns Starfish (COTS) using Convolutional Neural Network (CNN) and attention model. PLOS ONE. Retrieved from https://doi.org/10.1371/journal.pone.0283121\n",
        "\n",
        "Kaggle. (n.d.). TensorFlow Great Barrier Reef Competition. Retrieved from https://www.kaggle.com/competitions/tensorflow-great-barrier-reef\n",
        "\n",
        "Liu, J., Kusy, B., Marchant, R., Do, B., Merz, T., Crosswell, J., Steven, A., Heaney, N., von Richter, K., Tychsen-Smith, L., Ahmedt-Aristizabal, D., Armin, M. A., Carlin, G., Babcock, R., Moghadam, P., Smith, D., Davis, T., El Moujahid, K., Wicke, M., & Malpani, M. (2021, November 29). The CSIRO Crown-of-Thorn Starfish Detection Dataset. arXiv:2111.14311v1 [cs.CV]. Retrieved from https://arxiv.org/pdf/2111.14311.pdf\n",
        "\n",
        "Lv, W., Zhao, Y., Xu, S., Wei, J., Wang, G., Cui, C., Du, Y., Dang, Q., Liu, Y. (2023, July 6). DETRs Beat YOLOs on Real-time Object Detection. Baidu Inc. arXiv:2304.08069v2 [cs.CV]. Retrieved from https://arxiv.org/pdf/2304.08069v2.pdf\n",
        "\n",
        "outrunner. (n.d.). 4th Place Solution - CenterNet. Retrieved from https://www.kaggle.com/c/31703/discussion/307626\n",
        "\n",
        "Psi, & Babakhin, Y. (n.d.). 3rd place solution - Team Hydrogen. Retrieved from https://www.kaggle.com/c/31703/discussion/307707\n",
        "\n",
        "Figure 1: An Illustration of Real-Time Object Detection. I designed it using Canva.\n",
        "\n",
        "Figure 2: Crown-of-Thorns Starfish (COTS). Retrieved from https://user-images.githubusercontent.com/51321172/183879687-e5201d27-5b78-4b88-8e2c-887a51110f2f.jpg.\n",
        "\n",
        "Table 1: Writeups Summarizations. Summarized Kaggle winning solutions details for the TensorFlow - Help Protect the Great Barrier Reef competition.\n",
        "\n",
        "Note: Google search results and ChatGPT was used for some clarifications, knowledge gaining and formatting references."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "hJLTR37JiZ6_",
        "outputId": "0e445dac-ddeb-425b-aadc-a8ad460db960"
      },
      "id": "hJLTR37JiZ6_",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24ba45df",
      "metadata": {
        "papermill": {
          "duration": 0.014411,
          "end_time": "2023-07-16T01:54:11.636266",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.621855",
          "status": "completed"
        },
        "tags": [],
        "id": "24ba45df"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "Appendix A : Finding the best 5 writeups related to Object Detection using 2023-kaggle-ai-report Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d8dc314a",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2023-07-16T01:54:11.668349Z",
          "iopub.status.busy": "2023-07-16T01:54:11.667456Z",
          "iopub.status.idle": "2023-07-16T01:54:11.680791Z",
          "shell.execute_reply": "2023-07-16T01:54:11.680052Z"
        },
        "papermill": {
          "duration": 0.031953,
          "end_time": "2023-07-16T01:54:11.682913",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.650960",
          "status": "completed"
        },
        "tags": [],
        "id": "d8dc314a"
      },
      "outputs": [],
      "source": [
        "# Import Pandas library.\n",
        "import pandas as pd\n",
        "\n",
        "# Read the Kaggle writeups csv.\n",
        "#writeups = pd.read_csv('/kaggle/input/2023-kaggle-ai-report/kaggle_writeups_20230510.csv')\n",
        "\n",
        "# This is the first look for our dataframe writeups. You can see competition launch dates, competition titles, writeups details with URLs.\n",
        "#writeups.head(10)\n",
        "\n",
        "# Let's look a summary for the dataframe writeups. You can see Feedback Prize - English Language Learning competition and freq as 45.\n",
        "#writeups.describe()\n",
        "\n",
        "# I checked for the competitions and their writeups count.\n",
        "#writeups[['Title of Competition']].value_counts()\n",
        "\n",
        "# Checked for the Competition Launch Date counts as well.\n",
        "#writeups[['Competition Launch Date']].value_counts()\n",
        "\n",
        "# I found out there are competitions from many past years like 2012,2015,2019...etc. I needed to get only the past two year competitions.\n",
        "# Because the optimal way to find the recent advancements or the latest past two years learnings of the Kaggle community is to check with\n",
        "# the competitions launched and completed in past two years.\n",
        "\n",
        "# Therefore,\n",
        "# Roughly we should get 2 years data between May 11, 2021 to May 11, 2023 (2023 Kaggle AI Report competition launched date).\n",
        "\n",
        "# To work with dates we should convert original dates format in csv file into datetime format using standard pandas function .to_datetime().\n",
        "#writeups['Competition Launch Date'] = pd.to_datetime(writeups['Competition Launch Date'])\n",
        "#writeups['Date of Writeup'] = pd.to_datetime(writeups['Date of Writeup'])\n",
        "\n",
        "# After converting both competition launch date and date of writeup columns I got below.\n",
        "#writeups.head()\n",
        "\n",
        "# I firstly filtered the competitions and got competition details after 2021-05-11.\n",
        "#filtered_df = writeups[writeups['Competition Launch Date'] >= '2021-05-11']\n",
        "\n",
        "# Again for the better I filtered with date of writeups as well. Therefore you can see only the writeup details for the past two years.\n",
        "#data = filtered_df[filtered_df['Date of Writeup'] >= '2021-05-11']\n",
        "\n",
        "#data.head()\n",
        "\n",
        "# These are the writeup counts for competitions.\n",
        "#data[['Title of Competition']].value_counts()\n",
        "\n",
        "# I got interested on the TensorFlow - Help Protect the Great Barrier Reef competition (Kaggle,n.d.) because it is related to Image and/or Video data\n",
        "# and for the real-time object detection also it was a help for protecting the Great Barrier Reef.\n",
        "\n",
        "# I checked for the titles of writeups regarding the TensorFlow - Help Protect the Great Barrier Reef competition. There are writeups\n",
        "# from Kaggle teams who performed really well in this competition. Therefore those writeups should be definitely with good solutions.\n",
        "\n",
        "#data.loc[data['Title of Competition'] == 'TensorFlow - Help Protect the Great Barrier Reef']['Title of Writeup']\n",
        "\n",
        "# These are the portions of them.\n",
        "#data.loc[data['Title of Competition'] == 'TensorFlow - Help Protect the Great Barrier Reef']['Writeup']\n",
        "\n",
        "# I filtered the links for that writeups as well.\n",
        "#data.loc[data['Title of Competition'] == 'TensorFlow - Help Protect the Great Barrier Reef']['Writeup URL']\n",
        "\n",
        "# Finally I decided to find the past two years advancements for the real-time object detection via analyzing these findings combining\n",
        "# with the learnings from three research articles I found from Google search related to the real-time object detection."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc89b645",
      "metadata": {
        "papermill": {
          "duration": 0.014663,
          "end_time": "2023-07-16T01:54:11.712247",
          "exception": false,
          "start_time": "2023-07-16T01:54:11.697584",
          "status": "completed"
        },
        "tags": [],
        "id": "bc89b645"
      },
      "source": [
        "Appendix B : Generating submission file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Pandas library.\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Update the path to your local file location\n",
        "# If sample_submission.csv is in the current directory, you can directly use it.\n",
        "# Otherwise, update the file_path to its actual location.\n",
        "# For example, if it's in a 'data' subfolder, you can use:\n",
        "# file_path = os.path.join(\"data\", \"sample_submission.csv\")\n",
        "file_path = \"sample_submission.csv\"\n",
        "\n",
        "# Ensure the file exists\n",
        "if os.path.exists(file_path):\n",
        "    submission = pd.read_csv(file_path)\n",
        "    submission.head()\n",
        "\n",
        "    submission.loc[0]['value']='Image and/or video data'\n",
        "    submission.loc[1]['value']='https://www.kaggle.com/code/roshinifernando/real-time-object-detection'\n",
        "    submission.loc[2]['value']='https://www.kaggle.com/code/dokster/image-and-video-data-kaggle-ai-report/comments#2337671'\n",
        "    submission.loc[3]['value']='https://www.kaggle.com/code/bharatijog/video-image-data-and-ai-an-overview-of-recent/comments#2340870'\n",
        "    submission.loc[4]['value']='https://www.kaggle.com/code/soumyaoruganti/image-segmentation-ai-report-2023/comments#2342329'\n",
        "\n",
        "    submission.head()\n",
        "    submission.to_csv('submission_8.csv',index = False)\n",
        "else:\n",
        "    print(f\"Error: File not found at {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFEM7Oh0dBVF",
        "outputId": "c9b3512a-0842-452d-dfb8-df1797f09504"
      },
      "id": "RFEM7Oh0dBVF",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: File not found at sample_submission.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 12.085007,
      "end_time": "2023-07-16T01:54:12.609036",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-07-16T01:54:00.524029",
      "version": "2.4.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}